{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "import sqlite3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlitefile = 'data.sqlite'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(sqlitefile)\n",
    "c = conn.cursor()\n",
    "c.execute('''CREATE TABLE sanders\n",
    "       (datetime       REAL,\n",
    "       sentiment      REAL,\n",
    "       tweetID        TEXT);''')\n",
    "c.execute('''CREATE TABLE trump\n",
    "       (datetime       REAL,\n",
    "       sentiment      REAL,\n",
    "       tweetID        TEXT);''')\n",
    "c.execute('''CREATE TABLE clinton\n",
    "       (datetime       REAL,\n",
    "       sentiment      REAL,\n",
    "       tweetID        TEXT);''')\n",
    "c.execute('''CREATE TABLE cruz\n",
    "       (datetime       REAL,\n",
    "       sentiment      REAL,\n",
    "       tweetID        TEXT);''')\n",
    "c.execute('''CREATE TABLE unknown\n",
    "       (datetime       REAL,\n",
    "       sentiment      REAL,\n",
    "       tweetID        TEXT);''')\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "candidates = ['sanders', 'trump', 'clinton', 'cruz', 'unknown']\n",
    "for candidate in candidates:\n",
    "    data = pd.read_csv('data_'+candidate+'_00000.csv', header=None,\n",
    "                   names=['date', 'sentiment', 'tweetID'],\n",
    "                   dtype={'date': np.float64, 'sentiment': np.float64, 'tweetID': str})\n",
    "    conn = sqlite3.connect(sqlitefile)\n",
    "    c = conn.cursor()\n",
    "    c.execute('PRAGMA journal_mode=wal')\n",
    "    c.executemany('''INSERT INTO '''+candidate+'''(datetime, sentiment, tweetID) VALUES (?, ?, ?)''', np.array(data))\n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create downsampled database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlitefile_downsampled = 'data_downsampled.sqlite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(sqlitefile_downsampled)\n",
    "c = conn.cursor()\n",
    "c.execute('''CREATE TABLE sanders\n",
    "       (datetime      REAL,\n",
    "       sentiment      REAL,\n",
    "       tweet_count    INTEGER);''')\n",
    "c.execute('''CREATE TABLE trump\n",
    "       (datetime      REAL,\n",
    "       sentiment      REAL,\n",
    "       tweet_count    INTEGER);''')\n",
    "c.execute('''CREATE TABLE clinton\n",
    "       (datetime      REAL,\n",
    "       sentiment      REAL,\n",
    "       tweet_count    INTEGER);''')\n",
    "c.execute('''CREATE TABLE cruz\n",
    "       (datetime      REAL,\n",
    "       sentiment      REAL,\n",
    "       tweet_count    INTEGER);''')\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# Downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bin_size = 30 * 60 # 30 minutes, in seconds\n",
    "min_tweets = 100 # minimum number of tweets for a valid entry\n",
    "\n",
    "candidates = ['sanders', 'trump', 'clinton', 'cruz']\n",
    "\n",
    "for candidate in candidates:\n",
    "    \n",
    "    conn = sqlite3.connect(sqlitefile_downsampled)\n",
    "    c = conn.cursor()\n",
    "#     c.execute('''DELETE FROM '''+candidate+''' WHERE 1=1''')\n",
    "#     conn.commit()\n",
    "    c.execute('''SELECT * FROM '''+candidate+''' LIMIT 1;''')\n",
    "    row = c.fetchone()\n",
    "    conn.close()\n",
    "    \n",
    "    if not row:\n",
    "        # table is empty, start from scratch\n",
    "\n",
    "        conn = sqlite3.connect(sqlitefile)\n",
    "        c = conn.cursor()\n",
    "        c.execute('''SELECT * FROM '''+candidate+''';''')\n",
    "        all_rows = c.fetchall()\n",
    "        conn.close()\n",
    "        \n",
    "        prev_time = None\n",
    "        sentiments = []\n",
    "        tweet_count = 0\n",
    "        for row in all_rows:\n",
    "            sentiments.append(row[1])\n",
    "            tweet_count += 1\n",
    "            if not prev_time:\n",
    "                prev_time = row[0]\n",
    "                continue\n",
    "            time = row[0]\n",
    "            if time - prev_time > bin_size:\n",
    "                # we've passed bin_size, wrap it up\n",
    "                if time - prev_time > bin_size*2:\n",
    "                    # more than 2 bin_sizes have passed, we're missing data. Add an empty entry.\n",
    "                    conn = sqlite3.connect(sqlitefile_downsampled)\n",
    "                    c = conn.cursor()\n",
    "                    c.execute('PRAGMA journal_mode=wal')\n",
    "                    c.execute('''INSERT INTO '''+candidate+'''(datetime, sentiment, tweet_count) VALUES (?,?,?);''',\n",
    "                             (time - (time-prev_time)/2, None, tweet_count))\n",
    "                    conn.commit()\n",
    "                    conn.close()\n",
    "                    prev_time = time\n",
    "                    sentiments = []\n",
    "                    tweet_count = 0\n",
    "                elif tweet_count >= min_tweets: # check if we have a reasonable number of tweets to get a mean sentiment from\n",
    "                    conn = sqlite3.connect(sqlitefile_downsampled)\n",
    "                    c = conn.cursor()\n",
    "                    c.execute('PRAGMA journal_mode=wal')\n",
    "                    c.execute('''INSERT INTO '''+candidate+'''(datetime, sentiment, tweet_count) VALUES (?,?,?);''',\n",
    "                             (time - (time-prev_time)/2, np.mean(sentiments), tweet_count))\n",
    "                    conn.commit()\n",
    "                    conn.close()\n",
    "                    prev_time = time\n",
    "                    sentiments = []\n",
    "                    tweet_count = 0\n",
    "        \n",
    "    else: # table is not empty\n",
    "        \n",
    "        conn = sqlite3.connect(sqlitefile_downsampled)\n",
    "        c = conn.cursor()\n",
    "        c.execute('''SELECT datetime FROM '''+candidate+''' ORDER BY rowid DESC LIMIT 1;''')\n",
    "        row = c.fetchone()\n",
    "        downsampled_lasttime = row[0]\n",
    "        conn.close()\n",
    "        \n",
    "        conn = sqlite3.connect(sqlitefile)\n",
    "        c = conn.cursor()\n",
    "        c.execute('''SELECT datetime FROM '''+candidate+''' ORDER BY rowid DESC LIMIT 1;''')\n",
    "        row = c.fetchone()\n",
    "        data_lasttime = row[0]\n",
    "        conn.close()\n",
    "        \n",
    "        if data_lasttime - downsampled_lasttime > bin_size:\n",
    "            \n",
    "            # time to add another datapoint\n",
    "\n",
    "            conn = sqlite3.connect(sqlitefile)\n",
    "            c = conn.cursor()\n",
    "            c.execute('''SELECT * FROM '''+candidate+''' WHERE datetime > '''+str(downsampled_lasttime)+''';''')\n",
    "            rows = c.fetchall()\n",
    "            conn.close()\n",
    "\n",
    "            prev_time = downsampled_lasttime\n",
    "            sentiments = []\n",
    "            tweet_count = 0\n",
    "            for row in rows:\n",
    "                sentiments.append(row[1])\n",
    "                tweet_count += 1\n",
    "                if not prev_time:\n",
    "                    prev_time = row[0]\n",
    "                    continue\n",
    "                time = row[0]\n",
    "                if time - prev_time > bin_size:\n",
    "                    # we've passed bin_size, wrap it up\n",
    "                    if time - prev_time > bin_size*2:\n",
    "                        # more than 2 bin_sizes have passed, we're missing data. Add an empty entry.\n",
    "                        conn = sqlite3.connect(sqlitefile_downsampled)\n",
    "                        c = conn.cursor()\n",
    "                        c.execute('PRAGMA journal_mode=wal')\n",
    "                        c.execute('''INSERT INTO '''+candidate+'''(datetime, sentiment, tweet_count) VALUES (?,?,?);''',\n",
    "                                 (time - (time-prev_time)/2, None, tweet_count))\n",
    "                        conn.commit()\n",
    "                        conn.close()\n",
    "                    elif tweet_count >= min_tweets: # check if we have a reasonable number of tweets to get a mean sentiment from\n",
    "                        conn = sqlite3.connect(sqlitefile_downsampled)\n",
    "                        c = conn.cursor()\n",
    "                        c.execute('PRAGMA journal_mode=wal')\n",
    "                        c.execute('''INSERT INTO '''+candidate+'''(datetime, sentiment, tweet_count) VALUES (?,?,?);''',\n",
    "                                 (time - (time-prev_time)/2, np.mean(sentiments), tweet_count))\n",
    "                        conn.commit()\n",
    "                        conn.close()\n",
    "                    \n",
    "                    prev_time = time\n",
    "                    sentiments = []\n",
    "                    tweet_count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viral tweet detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentiment_diff_threshold = 0.071834651618471426 # two times the standard deviation of all data up to April 16th 2016\n",
    "# tps_diff_threshold = 4259.3176542475112 # two times the standard deviation of all data up to April 16th 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "candidate = 'sanders'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# c.execute('''\n",
    "#     SELECT datetime, tweetID FROM '''+candidate+'''\n",
    "#     WHERE datetime > '''+str(p[0])+'''\n",
    "#     AND datetime < '''+str(p[1])+'''\n",
    "#     GROUP BY tweetID\n",
    "#     HAVING COUNT(*) = (\n",
    "#                        SELECT MAX(Cnt) \n",
    "#                        FROM(\n",
    "#                              SELECT COUNT(*) as Cnt\n",
    "#                              FROM '''+candidate+'''\n",
    "#                              GROUP BY tweetID\n",
    "#                             ) tmp\n",
    "#                         )\n",
    "#     LIMIT 1;\n",
    "#     ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def diff_smooth(x, n):\n",
    "    a = np.zeros_like(x)\n",
    "    for i in range(len(x)-n):\n",
    "        i += n\n",
    "        a[i] = np.nanmean(x[i:i+n], axis=0) - np.nanmean(x[i-n:i], axis=0)\n",
    "    return a\n",
    "\n",
    "def get_peaks(x, threshold):\n",
    "    peaks = []\n",
    "    peaking = False\n",
    "    start = None\n",
    "    for a in x:\n",
    "        if not peaking:\n",
    "            if a[1] > threshold:\n",
    "                start = a[0]\n",
    "                peaking = True\n",
    "        elif peaking:\n",
    "            if a[1] < threshold:\n",
    "                end = a[0]\n",
    "                peaks.append([start, end])\n",
    "                start = None\n",
    "                peaking = False\n",
    "    return np.array(peaks)\n",
    "\n",
    "candidates = ['sanders', 'trump', 'clinton', 'cruz']\n",
    "viral_tweets = {}\n",
    "\n",
    "for candidate in candidates:\n",
    "\n",
    "    # get downsampled data\n",
    "    conn = sqlite3.connect(sqlitefile_downsampled)\n",
    "    c = conn.cursor()\n",
    "    c.execute('''SELECT * FROM '''+candidate+''';''')\n",
    "    rows = c.fetchall()\n",
    "    conn.close()\n",
    "\n",
    "    rows = np.array(rows).astype(np.float32)\n",
    "\n",
    "    # calculate derivative of sentiment and tweets per bin\n",
    "    data_diff = rows.copy()\n",
    "    data_diff[:,1] = np.abs(diff_smooth(rows[:,1], 10))\n",
    "    data_diff[:,2] = diff_smooth(rows[:,2], 2)\n",
    "\n",
    "    # get peaks above a threshold\n",
    "    sentiment_diff_threshold = np.std(data_diff[:,1])*3\n",
    "    tps_diff_threshold = np.std(data_diff[:,2])*3\n",
    "    sentiment_diff_peaks = get_peaks(data_diff[:,[0,1]], sentiment_diff_threshold)\n",
    "    tps_diff_peaks = get_peaks(data_diff[:,[0,2]], tps_diff_threshold)\n",
    "\n",
    "    # find most common tweetID of each peak\n",
    "    peaks = np.sort(np.vstack((sentiment_diff_peaks, tps_diff_peaks)), axis=0)\n",
    "    top_tweets = []\n",
    "    prev_top_tweet = None\n",
    "    for p in peaks:\n",
    "        \n",
    "        # get most common tweetID during the peak\n",
    "        conn = sqlite3.connect(sqlitefile)\n",
    "        c = conn.cursor()\n",
    "        c.execute('''\n",
    "        SELECT datetime, tweetID FROM '''+candidate+'''\n",
    "        WHERE datetime > '''+str('%f' % p[0])+'''\n",
    "        AND datetime < '''+str('%f' % p[1])+''';\n",
    "        ''')\n",
    "        rows = c.fetchall()\n",
    "        conn.close()\n",
    "\n",
    "        rows = np.array(rows)\n",
    "        \n",
    "        if len(rows) < 1:\n",
    "            continue\n",
    "\n",
    "        (values,counts) = np.unique(rows[:,1],return_counts=True)\n",
    "        ind=np.argmax(counts)\n",
    "        top_tweet = values[ind]\n",
    "\n",
    "        if top_tweet == None:\n",
    "            continue\n",
    "\n",
    "        # check if same tweet as previous peak\n",
    "        if prev_top_tweet and top_tweet == prev_top_tweet:\n",
    "            continue\n",
    "        prev_top_tweet = top_tweet\n",
    "        top_tweets.append({'datetime': int(np.mean(p)), 'tweetID': top_tweet})\n",
    "    \n",
    "    viral_tweets[candidate] = top_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('viraltweets.json', 'w') as f:\n",
    "    json.dump(viral_tweets, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
