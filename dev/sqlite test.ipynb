{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "import sqlite3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlitefile = 'data.sqlite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlitefile_downsampled = '../../jeroendelcour.nl/2016election/data_downsampled.sqlite'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(sqlitefile)\n",
    "c = conn.cursor()\n",
    "c.execute('''CREATE TABLE sanders\n",
    "       (datetime       REAL,\n",
    "       sentiment      REAL,\n",
    "       tweetID        TEXT);''')\n",
    "c.execute('''CREATE TABLE trump\n",
    "       (datetime       REAL,\n",
    "       sentiment      REAL,\n",
    "       tweetID        TEXT);''')\n",
    "c.execute('''CREATE TABLE clinton\n",
    "       (datetime       REAL,\n",
    "       sentiment      REAL,\n",
    "       tweetID        TEXT);''')\n",
    "c.execute('''CREATE TABLE cruz\n",
    "       (datetime       REAL,\n",
    "       sentiment      REAL,\n",
    "       tweetID        TEXT);''')\n",
    "c.execute('''CREATE TABLE unknown\n",
    "       (datetime       REAL,\n",
    "       sentiment      REAL,\n",
    "       tweetID        TEXT);''')\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "candidates = ['sanders', 'trump', 'clinton', 'cruz', 'unknown']\n",
    "for candidate in candidates:\n",
    "    data = pd.read_csv('data_'+candidate+'_00000.csv', header=None,\n",
    "                   names=['date', 'sentiment', 'tweetID'],\n",
    "                   dtype={'date': np.float64, 'sentiment': np.float64, 'tweetID': str})\n",
    "    conn = sqlite3.connect(sqlitefile)\n",
    "    c = conn.cursor()\n",
    "    c.execute('PRAGMA journal_mode=wal')\n",
    "    c.executemany('''INSERT INTO '''+candidate+'''(datetime, sentiment, tweetID) VALUES (?, ?, ?)''', np.array(data))\n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create downsampled database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(sqlitefile_downsampled)\n",
    "c = conn.cursor()\n",
    "c.execute('''CREATE TABLE sanders\n",
    "       (datetime      REAL,\n",
    "       sentiment      REAL,\n",
    "       tweet_count    INTEGER);''')\n",
    "c.execute('''CREATE TABLE trump\n",
    "       (datetime      REAL,\n",
    "       sentiment      REAL,\n",
    "       tweet_count    INTEGER);''')\n",
    "c.execute('''CREATE TABLE clinton\n",
    "       (datetime      REAL,\n",
    "       sentiment      REAL,\n",
    "       tweet_count    INTEGER);''')\n",
    "c.execute('''CREATE TABLE cruz\n",
    "       (datetime      REAL,\n",
    "       sentiment      REAL,\n",
    "       tweet_count    INTEGER);''')\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "# Downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bin_size = 60 * 60 * 2 # 2 hours\n",
    "min_tweets = 1000 # minimum number of tweets for a valid entry\n",
    "\n",
    "candidates = ['sanders', 'trump', 'clinton', 'cruz']\n",
    "\n",
    "for candidate in candidates:\n",
    "    \n",
    "    conn = sqlite3.connect(sqlitefile_downsampled)\n",
    "    c = conn.cursor()\n",
    "#     c.execute('''DELETE FROM '''+candidate+''' WHERE 1=1''')\n",
    "#     conn.commit()\n",
    "    c.execute('''SELECT * FROM '''+candidate+''' LIMIT 1;''')\n",
    "    row = c.fetchone()\n",
    "    conn.close()\n",
    "    \n",
    "    if not row:\n",
    "        # table is empty, start from scratch\n",
    "\n",
    "        conn = sqlite3.connect(sqlitefile)\n",
    "        c = conn.cursor()\n",
    "        c.execute('''SELECT * FROM '''+candidate+''';''')\n",
    "        all_rows = c.fetchall()\n",
    "        conn.close()\n",
    "        \n",
    "        prev_time = None\n",
    "        sentiments = []\n",
    "        tweet_count = 0\n",
    "        for row in all_rows:\n",
    "            sentiments.append(row[1])\n",
    "            tweet_count += 1\n",
    "            if not prev_time:\n",
    "                prev_time = row[0]\n",
    "                continue\n",
    "            time = row[0]\n",
    "            if time - prev_time > bin_size:\n",
    "                # we've passed bin_size, wrap it up\n",
    "                if time - prev_time > bin_size*2:\n",
    "                    # more than 2 bin_sizes have passed, we're missing data. Add an empty entry.\n",
    "                    conn = sqlite3.connect(sqlitefile_downsampled)\n",
    "                    c = conn.cursor()\n",
    "                    c.execute('PRAGMA journal_mode=wal')\n",
    "                    c.execute('''INSERT INTO '''+candidate+'''(datetime, sentiment, tweet_count) VALUES (?,?,?);''',\n",
    "                             (time - (time-prev_time)/2, None, tweet_count))\n",
    "                    conn.commit()\n",
    "                    conn.close()\n",
    "                    prev_time = time\n",
    "                    sentiments = []\n",
    "                    tweet_count = 0\n",
    "                elif tweet_count >= min_tweets: # check if we have a reasonable number of tweets to get a mean sentiment from\n",
    "                    conn = sqlite3.connect(sqlitefile_downsampled)\n",
    "                    c = conn.cursor()\n",
    "                    c.execute('PRAGMA journal_mode=wal')\n",
    "                    c.execute('''INSERT INTO '''+candidate+'''(datetime, sentiment, tweet_count) VALUES (?,?,?);''',\n",
    "                             (time - (time-prev_time)/2, np.mean(sentiments), tweet_count))\n",
    "                    conn.commit()\n",
    "                    conn.close()\n",
    "                    prev_time = time\n",
    "                    sentiments = []\n",
    "                    tweet_count = 0\n",
    "        \n",
    "    else: # table is not empty\n",
    "        \n",
    "        conn = sqlite3.connect(sqlitefile_downsampled)\n",
    "        c = conn.cursor()\n",
    "        c.execute('''SELECT datetime FROM '''+candidate+''' ORDER BY rowid DESC LIMIT 1;''')\n",
    "        row = c.fetchone()\n",
    "        downsampled_lasttime = row[0]\n",
    "        conn.close()\n",
    "        \n",
    "        conn = sqlite3.connect(sqlitefile)\n",
    "        c = conn.cursor()\n",
    "        c.execute('''SELECT datetime FROM '''+candidate+''' ORDER BY rowid DESC LIMIT 1;''')\n",
    "        row = c.fetchone()\n",
    "        data_lasttime = row[0]\n",
    "        conn.close()\n",
    "        \n",
    "        if data_lasttime - downsampled_lasttime > bin_size:\n",
    "            \n",
    "            # time to add another datapoint\n",
    "\n",
    "            conn = sqlite3.connect(sqlitefile)\n",
    "            c = conn.cursor()\n",
    "            c.execute('''SELECT * FROM '''+candidate+''' WHERE datetime > '''+str(downsampled_lasttime)+''';''')\n",
    "            rows = c.fetchall()\n",
    "            conn.close()\n",
    "\n",
    "            prev_time = downsampled_lasttime\n",
    "            sentiments = []\n",
    "            tweet_count = 0\n",
    "            for row in rows:\n",
    "                sentiments.append(row[1])\n",
    "                tweet_count += 1\n",
    "                if not prev_time:\n",
    "                    prev_time = row[0]\n",
    "                    continue\n",
    "                time = row[0]\n",
    "                if time - prev_time > bin_size:\n",
    "                    # we've passed bin_size, wrap it up\n",
    "                    if time - prev_time > bin_size*2:\n",
    "                        # more than 2 bin_sizes have passed, we're missing data. Add an empty entry.\n",
    "                        conn = sqlite3.connect(sqlitefile_downsampled)\n",
    "                        c = conn.cursor()\n",
    "                        c.execute('PRAGMA journal_mode=wal')\n",
    "                        c.execute('''INSERT INTO '''+candidate+'''(datetime, sentiment, tweet_count) VALUES (?,?,?);''',\n",
    "                                 (time - (time-prev_time)/2, None, tweet_count))\n",
    "                        conn.commit()\n",
    "                        conn.close()\n",
    "                    elif tweet_count >= min_tweets: # check if we have a reasonable number of tweets to get a mean sentiment from\n",
    "                        conn = sqlite3.connect(sqlitefile_downsampled)\n",
    "                        c = conn.cursor()\n",
    "                        c.execute('PRAGMA journal_mode=wal')\n",
    "                        c.execute('''INSERT INTO '''+candidate+'''(datetime, sentiment, tweet_count) VALUES (?,?,?);''',\n",
    "                                 (time - (time-prev_time)/2, np.mean(sentiments), tweet_count))\n",
    "                        conn.commit()\n",
    "                        conn.close()\n",
    "                    \n",
    "                    prev_time = time\n",
    "                    sentiments = []\n",
    "                    tweet_count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viral tweet detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "def hide_spines():\n",
    "    \"\"\"Hides the top and rightmost axis spines from view for all active\n",
    "    figures and their respective axes.\"\"\"\n",
    "\n",
    "    # Retrieve a list of all current figures.\n",
    "    figures = [x for x in matplotlib._pylab_helpers.Gcf.get_all_fig_managers()]\n",
    "    for figure in figures:\n",
    "        # Get all Axis instances related to the figure.\n",
    "        for ax in figure.canvas.figure.get_axes():\n",
    "            # Disable spines.\n",
    "            ax.spines['right'].set_color('none')\n",
    "            ax.spines['top'].set_color('none')\n",
    "            ax.spines['left'].set_color('none')\n",
    "            ax.spines['bottom'].set_color('none')\n",
    "            # Disable ticks.\n",
    "            ax.xaxis.set_ticks_position('bottom')\n",
    "            ax.yaxis.set_ticks_position('left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def diff_smooth(x, n):\n",
    "    a = np.zeros_like(x)\n",
    "    for i in range(int(round(n/2)),len(x)-n):\n",
    "        i += n\n",
    "        a[i] = np.nanmean(x[i-int(round(n/2)):i+int(round(n/2))], axis=0) - np.nanmean(x[i-n:i], axis=0)\n",
    "    return a\n",
    "\n",
    "def get_peaks(x, threshold):\n",
    "    peaks = []\n",
    "    peaking = False\n",
    "    start = None\n",
    "    for a in x:\n",
    "        if not peaking:\n",
    "            if a[1] > threshold:\n",
    "                start = a[0]\n",
    "                peaking = True\n",
    "        elif peaking:\n",
    "            if a[1] < threshold:\n",
    "                end = a[0]\n",
    "                peaks.append([start, end])\n",
    "                start = None\n",
    "                peaking = False\n",
    "    return np.array(peaks)\n",
    "\n",
    "candidates = ['sanders', 'trump', 'cruz', 'clinton']\n",
    "viral_tweets = {}\n",
    "\n",
    "for candidate in candidates:\n",
    "\n",
    "    # get downsampled data\n",
    "    conn = sqlite3.connect(sqlitefile_downsampled)\n",
    "    c = conn.cursor()\n",
    "    c.execute('''SELECT * FROM '''+candidate+''' WHERE datetime > 1459456469;''')\n",
    "    # 1459456469 is the epoch time after which I started recording tweetIDs\n",
    "    rows = c.fetchall()\n",
    "    conn.close()\n",
    "\n",
    "    rows = np.array(rows).astype(np.float32)\n",
    "    \n",
    "    data_downsampled = rows.copy()\n",
    "\n",
    "    # calculate derivative of sentiment and tweets per bin\n",
    "    data_diff = rows.copy()\n",
    "    data_diff[:,1] = np.abs(diff_smooth(rows[:,1], 2))\n",
    "    data_diff[:,2] = diff_smooth(rows[:,2], 1)\n",
    "\n",
    "    # get peaks above a threshold\n",
    "    sentiment_diff_threshold = np.std(data_diff[:,1])*3\n",
    "    tps_diff_threshold = np.std(data_diff[:,2])*3\n",
    "    sentiment_diff_peaks = get_peaks(data_diff[:,[0,1]], sentiment_diff_threshold)\n",
    "    tps_diff_peaks = get_peaks(data_diff[:,[0,2]], tps_diff_threshold)\n",
    "    \n",
    "#     plot\n",
    "    plt.figure(figsize=(16,9))\n",
    "    plt.subplot(411)\n",
    "    plt.title(candidate)\n",
    "    plt.plot(data_downsampled[:,0], data_downsampled[:,1])\n",
    "    for p in sentiment_diff_peaks:\n",
    "        plt.axvline(x=p[0], ymin=0.95, ymax=1, c='r')\n",
    "    for p in tps_diff_peaks:\n",
    "        plt.axvline(x=p[0], ymin=0.9, ymax=0.95, c='r')\n",
    "    plt.ylim([-0.5,0.5])\n",
    "    plt.subplot(412)\n",
    "    plt.plot(data_downsampled[:,0], data_downsampled[:,2])\n",
    "    plt.subplot(413)\n",
    "    plt.plot(data_diff[:,0], data_diff[:,1])\n",
    "    plt.axhline(y=sentiment_diff_threshold, c='r')\n",
    "    plt.subplot(414)\n",
    "    plt.plot(data_diff[:,0], data_diff[:,2])\n",
    "    plt.axhline(y=tps_diff_threshold, c='r')\n",
    "    hide_spines()\n",
    "    # plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # find most common tweetID of each peak\n",
    "    if len(sentiment_diff_peaks) >= 1:\n",
    "        if len(tps_diff_peaks) >= 1:\n",
    "            peaks = np.sort(np.vstack((sentiment_diff_peaks, tps_diff_peaks)), axis=0)\n",
    "        else:\n",
    "            peaks = sentiment_diff_peaks\n",
    "    else:\n",
    "        peaks = tps_diff_peaks\n",
    "    top_tweets = []\n",
    "    prev_top_tweet = None\n",
    "    for p in peaks:\n",
    "        \n",
    "        # get most common tweetID during the peak\n",
    "        conn = sqlite3.connect(sqlitefile)\n",
    "        c = conn.cursor()\n",
    "        c.execute('''\n",
    "        SELECT datetime, tweetID FROM '''+candidate+'''\n",
    "        WHERE datetime > '''+str('%f' % p[0])+'''\n",
    "        AND datetime < '''+str('%f' % p[1])+''';\n",
    "        ''')\n",
    "        rows = c.fetchall()\n",
    "        conn.close()\n",
    "\n",
    "        rows = np.array(rows)\n",
    "        \n",
    "        if len(rows) < 1:\n",
    "            continue\n",
    "\n",
    "        (values,counts) = np.unique(rows[:,1],return_counts=True)\n",
    "        ind=np.argmax(counts)\n",
    "        top_tweet = values[ind]\n",
    "\n",
    "        if top_tweet == None:\n",
    "            continue\n",
    "\n",
    "        # check if same tweet as previous peak\n",
    "        if prev_top_tweet and top_tweet == prev_top_tweet:\n",
    "            continue\n",
    "        prev_top_tweet = top_tweet\n",
    "        top_tweets.append({'datetime': int(np.mean(p)), 'tweetID': top_tweet})\n",
    "    \n",
    "    viral_tweets[candidate] = top_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../../jeroendelcour.nl/public/2016election/viraltweets.json', 'w') as f:\n",
    "    json.dump(viral_tweets, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
